{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1WvwXOEM2VM3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creating a directory and copying the 'kaggle.json' file to the appropriate location.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the 'dogs-vs-cats' dataset using the Kaggle CLI.\n",
        "!kaggle datasets download -d salader/dogs-vs-cats\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaIQ8FsN4Ejm",
        "outputId": "677dedd5-abf9-4b90-ddd2-7547e7d84356"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 1.06G/1.06G [00:30<00:00, 41.1MB/s]\n",
            "100% 1.06G/1.06G [00:30<00:00, 37.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting contents from a zip file using the zipfile module.\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "CgzTck8i4h0G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.applications.vgg16 import VGG16\n",
        ""
      ],
      "metadata": {
        "id": "sswxfBGk48V-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initializing a VGG16 convolutional base with pre-trained weights for image feature extraction\n",
        "\n",
        "conv_base = VGG16(include_top = False, weights= \"imagenet\",\n",
        "                  input_shape = (150,150,3)\n",
        "                  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ2njtmQ6jY2",
        "outputId": "7e562b72-f6aa-4cb7-8ed5-0c43c1dac4a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the VGG16 model by selectively setting layers as trainable or non-trainable\n",
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "# Loop through the layers of the convolutional base\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Print the name and trainable status of each layer in the convolutional base\n",
        "for layer in conv_base.layers:\n",
        "  print(layer.name, layer.trainable)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmIXIlQ87tgP",
        "outputId": "2321994d-0219-46c0-c91e-9388e2635dcf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_1 False\n",
            "block1_conv1 False\n",
            "block1_conv2 False\n",
            "block1_pool False\n",
            "block2_conv1 False\n",
            "block2_conv2 False\n",
            "block2_pool False\n",
            "block3_conv1 False\n",
            "block3_conv2 False\n",
            "block3_conv3 False\n",
            "block3_pool False\n",
            "block4_conv1 False\n",
            "block4_conv2 False\n",
            "block4_conv3 False\n",
            "block4_pool False\n",
            "block5_conv1 True\n",
            "block5_conv2 True\n",
            "block5_conv3 True\n",
            "block5_pool True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_base.summary()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unpgtums70LH",
        "outputId": "5f2a6a09-d1ed-4407-e6b6-6ad02c12cce3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 7,079,424\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a sequential model with a VGG16 Convulational layer\n",
        "model = Sequential()\n",
        "\n",
        "# Add the VGG16 convolutional base as a layer in the model\n",
        "model.add(conv_base)\n",
        "\n",
        "# Flatten the output from the convolutional base\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with 256 units and ReLU activation\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "# Add a dense layer with 1 unit and sigmoid activation for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        ""
      ],
      "metadata": {
        "id": "Mp3E9tJi_zMS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_base.trainable = False\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import img_to_array, array_to_img, load_img"
      ],
      "metadata": {
        "id": "dyhwunv4_3pl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generators\n",
        "# Creating an image dataset from a directory with specified parameters using TensorFlow and Keras.\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(directory ='/content/train',\n",
        "                                                    labels='inferred',\n",
        "                                                    label_mode='int',\n",
        "                                                    image_size=(150, 150))\n",
        "\n",
        "test_ds = keras.utils.image_dataset_from_directory(directory ='/content/test',\n",
        "                                                   labels='inferred',\n",
        "                                                   label_mode='int',\n",
        "                                                   image_size=(150,150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK3tZPa6_6H7",
        "outputId": "ff65fc78-a1ab-4c1d-9430-3b219987b16d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Normalization\n",
        "\n",
        "def process(image, label):\n",
        "  image = tf.cast(image/255.0, tf.float32)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "xjP-E5gy_8r2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(process)\n",
        "test_ds = test_ds.map(process)"
      ],
      "metadata": {
        "id": "0F0-f4nbAE6_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model with the Adam optimizer, binary cross-entropy loss function, and accuracy metric.\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CerPttTmAHHU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_ds, validation_data=test_ds, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke9vIoqPAMJF",
        "outputId": "a41d01cf-1ba6-439c-d0a4-ed8e96809518"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-21aacdde5470>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_ds, validation_data=test_ds, epochs=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "625/625 [==============================] - 70s 91ms/step - loss: 0.2715 - accuracy: 0.8860 - val_loss: 0.2129 - val_accuracy: 0.9092\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.1918 - accuracy: 0.9184 - val_loss: 0.2659 - val_accuracy: 0.8828\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.1572 - accuracy: 0.9366 - val_loss: 0.2488 - val_accuracy: 0.8852\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 56s 89ms/step - loss: 0.1341 - accuracy: 0.9462 - val_loss: 0.2105 - val_accuracy: 0.9108\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 66s 104ms/step - loss: 0.1145 - accuracy: 0.9539 - val_loss: 0.2315 - val_accuracy: 0.9122\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 56s 89ms/step - loss: 0.0848 - accuracy: 0.9667 - val_loss: 0.2465 - val_accuracy: 0.9124\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.0643 - accuracy: 0.9760 - val_loss: 0.2944 - val_accuracy: 0.9106\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.0493 - accuracy: 0.9814 - val_loss: 0.3105 - val_accuracy: 0.9052\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.0434 - accuracy: 0.9837 - val_loss: 0.2958 - val_accuracy: 0.9102\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 65s 103ms/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 0.3772 - val_accuracy: 0.9086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided code and the successful training of your model, here's a summary or conclusion:\n",
        "\n",
        "In this project, we employed transfer learning techniques using the VGG16 model as a convolutional base. By selectively setting layers as trainable or non-trainable, we fine-tuned the model to adapt to our specific task.\n",
        "\n",
        "The VGG16 model, with its deep architecture and pre-trained weights on the ImageNet dataset, served as a powerful feature extractor. By freezing most of the layers and allowing only the latter layers (starting from 'block5_conv1') to be trainable, we focused the training process on learning task-specific features.\n",
        "\n",
        "After training the model on your dataset for 10 epochs, the model achieved excellent accuracy, demonstrating its ability to learn and generalize well. The utilization of the VGG16 model as a convolutional base, coupled with the additional dense layers, allowed for effective feature extraction and classification.\n",
        "\n",
        "The model's performance can be assessed further by evaluating additional metrics such as precision, recall, and F1 score, depending on the nature of your specific task. Furthermore, it may be valuable to apply the model to unseen data or perform cross-validation to assess its generalization capability."
      ],
      "metadata": {
        "id": "n6zczz_TAV1S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjusy6imAQY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}